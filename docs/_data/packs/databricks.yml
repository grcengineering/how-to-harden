# AUTO-GENERATED by scripts/sync-packs-to-data.sh â€” do not edit manually
# Generated: 2026-02-18T04:04:58Z

"1.1":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-1.01-enforce-sso-with-mfa.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-1.01-enforce-sso-with-mfa.tf"
    excerpts:
      terraform:
        content: |
            # Enforce SSO-only login by disabling local password authentication
            resource "databricks_workspace_conf" "sso_enforcement" {
            custom_config = {
              "enableTokensConfig"     = false
              "enableIpAccessLists"    = var.profile_level >= 2
            }
            }

"1.2":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-1.02-service-principal-security.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-1.02-service-principal-security.tf"
    excerpts:
      terraform:
        content: |
            # Create purpose-specific service principals for automation
            resource "databricks_service_principal" "automation" {
            for_each = var.service_principals
          
            display_name = each.value.display_name
            active       = true
            }
          
            # Grant workspace-level permissions to service principals
            # Each service principal gets CAN_ATTACH_TO on designated clusters only
            resource "databricks_permissions" "service_principal_cluster_usage" {
            for_each = var.service_principals
          
            cluster_policy_id = databricks_cluster_policy.hardened.id
          
            access_control {
              service_principal_name = databricks_service_principal.automation[each.key].application_id
              permission_level       = "CAN_USE"
            }
            }

"1.3":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-1.03-ip-access-lists.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-1.03-ip-access-lists.tf"
    excerpts:
      terraform:
        content: |
            # Allowlist: Restrict workspace access to known corporate IP ranges (L2+)
            resource "databricks_ip_access_list" "allow_corporate" {
            count = var.profile_level >= 2 && length(var.allowed_ip_cidrs) > 0 ? 1 : 0
          
            label        = "HTH - Corporate Network Allow List"
            list_type    = "ALLOW"
            ip_addresses = var.allowed_ip_cidrs
          
            depends_on = [databricks_workspace_conf.sso_enforcement]
            }
          
            # Blocklist: Deny access from known-bad IP ranges (L2+)
            resource "databricks_ip_access_list" "block_bad_ips" {
            count = var.profile_level >= 2 && length(var.blocked_ip_cidrs) > 0 ? 1 : 0
          
            label        = "HTH - Blocked IP Ranges"
            list_type    = "BLOCK"
            ip_addresses = var.blocked_ip_cidrs
          
            depends_on = [databricks_workspace_conf.sso_enforcement]
            }

"2.1":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-2.01-data-governance.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-2.01-data-governance.tf"
    excerpts:
      terraform:
        content: |
            # Unity Catalog grants are managed via SQL statements.
            # This resource configures workspace-level Unity Catalog settings.
            resource "databricks_workspace_conf" "unity_catalog_governance" {
            custom_config = {
              "enableUnityCatalog" = true
            }
            }
          
            # Restrict catalog creation to admins only via SQL global config
            resource "databricks_sql_global_config" "governance" {
            security_policy = "DATA_ACCESS_CONTROL"
          
            data_access_config = {
              "spark.databricks.unityCatalog.enabled" = "true"
            }
            }

"2.2":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-2.02-data-masking.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-2.02-data-masking.tf"
    excerpts:
      terraform:
        content: |
            # Note: Dynamic data masking in Unity Catalog is configured via SQL statements
            # (CREATE FUNCTION + ALTER TABLE ... SET MASK). This control enforces the
            # workspace-level configuration that enables column masking support.
            #
            # The SQL masking functions below should be applied via databricks_sql_query
            # or a separate SQL migration pipeline:
            #
            #   CREATE FUNCTION production.masks.mask_ssn(ssn STRING)
            #   RETURNS STRING
            #   RETURN CASE
            #       WHEN is_account_group_member('pii_admin') THEN ssn
            #       ELSE CONCAT('XXX-XX-', RIGHT(ssn, 4))
            #   END;
            #
            #   ALTER TABLE production.customer_data.customers
            #   ALTER COLUMN ssn SET MASK production.masks.mask_ssn;
          
            # Enable table access control to support column-level masking (L2+)
            resource "databricks_workspace_conf" "data_masking" {
            count = var.profile_level >= 2 ? 1 : 0
          
            custom_config = {
              "enableTableAccessControl" = true
            }
            }

"2.3":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-2.03-audit-logging.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-2.03-audit-logging.tf"
    excerpts:
      terraform:
        content: |
            # Enable system tables for audit log access
            # System tables (system.access.audit) provide comprehensive audit logging
            # for all workspace events including data access, cluster operations, and
            # administrative changes.
            resource "databricks_workspace_conf" "audit_logging" {
            custom_config = {
              "enableDbfsFileBrowser" = false
              "enableExportNotebook"  = var.profile_level >= 3 ? false : true
            }
            }
          
            # Note: Verbose audit log queries should be scheduled via Databricks SQL:
            #
            #   SELECT event_time, user_identity.email as user_email,
            #          action_name, request_params.full_name_arg as table_accessed,
            #          source_ip_address
            #   FROM system.access.audit
            #   WHERE action_name IN ('getTable', 'commandSubmit')
            #     AND event_time > current_timestamp() - INTERVAL 24 HOURS
            #   ORDER BY event_time DESC;

"3.1":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-3.01-cluster-policies.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-3.01-cluster-policies.tf"
    excerpts:
      terraform:
        content: |
            # Hardened cluster policy enforcing approved runtimes, node types,
            # auto-termination, and init script restrictions
            resource "databricks_cluster_policy" "hardened" {
            name = "HTH - Hardened Cluster Policy"
          
            definition = jsonencode({
              "spark_version" = {
                "type"   = "allowlist"
                "values" = var.allowed_spark_versions
              }
              "node_type_id" = {
                "type"   = "allowlist"
                "values" = var.allowed_node_types
              }
              "autotermination_minutes" = {
                "type"         = "range"
                "minValue"     = 10
                "maxValue"     = var.autotermination_minutes_max
                "defaultValue" = var.autotermination_minutes_default
              }
              "custom_tags.Environment" = {
                "type"  = "fixed"
                "value" = "production"
              }
              "custom_tags.ManagedBy" = {
                "type"  = "fixed"
                "value" = "howtoharden"
              }
              "init_scripts" = {
                "type"  = "fixed"
                "value" = []
              }
              "enable_elastic_disk" = {
                "type"         = "fixed"
                "value"        = true
                "hidden"       = true
              }
            })
            }
          
            # Grant CAN_USE on the hardened policy to all users
            resource "databricks_permissions" "cluster_policy_usage" {
            cluster_policy_id = databricks_cluster_policy.hardened.id
          
            access_control {
              group_name       = "users"
              permission_level = "CAN_USE"
            }
            }

"3.2":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-3.02-network-isolation.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-3.02-network-isolation.tf"
    excerpts:
      terraform:
        content: |
            # Enforce no public IP addresses on cluster nodes (L2+)
            # This adds a cluster policy overlay that prevents public IP assignment
            resource "databricks_cluster_policy" "network_isolation" {
            count = var.profile_level >= 2 ? 1 : 0
          
            name = "HTH - Network Isolation Policy"
          
            definition = jsonencode({
              "enable_local_disk_encryption" = {
                "type"  = "fixed"
                "value" = true
              }
              "azure_attributes.availability" = {
                "type"         = "allowlist"
                "values"       = ["ON_DEMAND_AZURE"]
                "defaultValue" = "ON_DEMAND_AZURE"
              }
              "custom_tags.NetworkIsolation" = {
                "type"  = "fixed"
                "value" = "enabled"
              }
            })
            }
          
            # Grant CAN_USE on the network isolation policy (L2+)
            resource "databricks_permissions" "network_isolation_usage" {
            count = var.profile_level >= 2 ? 1 : 0
          
            cluster_policy_id = databricks_cluster_policy.network_isolation[0].id
          
            access_control {
              group_name       = "users"
              permission_level = "CAN_USE"
            }
            }
          
            # Note: Full VPC/VNet deployment with Private Link requires account-level
            # Terraform resources (databricks_mws_workspaces, databricks_mws_networks,
            # databricks_mws_private_access_settings). Those are provisioned at workspace
            # creation time and are outside the scope of workspace-level hardening.
            # See the guide section 3.2 for the account-level Terraform example.

"4.1":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-4.01-secret-scopes.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-4.01-secret-scopes.tf"
    excerpts:
      terraform:
        content: |
            # Create Databricks-backed secret scopes for credential storage
            resource "databricks_secret_scope" "managed" {
            for_each = var.secret_scopes
          
            name                     = each.key
            initial_manage_principal = each.value.initial_manage_principal
            }
          
            # Grant READ access to the data_engineers group on each secret scope
            resource "databricks_secret_acl" "data_engineers_read" {
            for_each = var.secret_scopes
          
            scope      = databricks_secret_scope.managed[each.key].name
            principal  = "data_engineers"
            permission = "READ"
            }

"4.2":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-4.02-external-secret-store.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-4.02-external-secret-store.tf"
    excerpts:
      terraform:
        content: |
            # Azure Key Vault-backed secret scope (L2+, Azure only)
            # Secrets are fetched directly from Key Vault at runtime rather than
            # stored in Databricks, providing centralized secret lifecycle management.
            resource "databricks_secret_scope" "azure_keyvault" {
            count = var.profile_level >= 2 && var.azure_keyvault_resource_id != "" ? 1 : 0
          
            name = "azure-kv-scope"
          
            keyvault_metadata {
              resource_id = var.azure_keyvault_resource_id
              dns_name    = var.azure_keyvault_dns_name
            }
            }
          
            # Note: For AWS deployments, use AWS Secrets Manager or Parameter Store
            # integration via instance profiles and IAM roles on the cluster. Configure
            # the cluster policy to enforce the required instance profile ARN.
            #
            # For GCP deployments, use Google Secret Manager via workload identity
            # federation configured on the cluster service account.

"5.1":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-5.01-security-monitoring.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-5.01-security-monitoring.tf"
    excerpts:
      terraform:
        content: |
            # Workspace configuration for security monitoring
            # Disables risky features that complicate audit trails
            resource "databricks_workspace_conf" "security_monitoring" {
            custom_config = {
              # Disable DBFS file browser to prevent unaudited file access
              "enableDbfsFileBrowser" = false
          
              # Restrict notebook export to prevent data exfiltration (L3)
              "enableExportNotebook" = var.profile_level >= 3 ? false : true
          
              # Disable results download for non-admin users (L2+)
              "enableResultsDownloading" = var.profile_level >= 2 ? false : true
            }
            }
          
            # Note: Detection queries should be scheduled as Databricks SQL alerts:
            #
            # Bulk data access detection:
            #   SELECT user_identity.email, request_params.full_name_arg as table_name,
            #          COUNT(*) as access_count
            #   FROM system.access.audit
            #   WHERE action_name = 'commandSubmit'
            #     AND event_time > current_timestamp() - INTERVAL 1 HOUR
            #   GROUP BY user_identity.email, request_params.full_name_arg
            #   HAVING COUNT(*) > 100;
            #
            # Unusual export detection:
            #   SELECT * FROM system.access.audit
            #   WHERE action_name IN ('downloadResults', 'exportResults')
            #     AND event_time > current_timestamp() - INTERVAL 24 HOURS;
            #
            # Service principal anomaly detection:
            #   SELECT user_identity.email, source_ip_address, COUNT(*) as request_count
            #   FROM system.access.audit
            #   WHERE user_identity.email LIKE 'svc-%'
            #     AND event_time > current_timestamp() - INTERVAL 1 HOUR
            #   GROUP BY user_identity.email, source_ip_address;

