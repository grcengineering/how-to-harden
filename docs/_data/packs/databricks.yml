# AUTO-GENERATED by scripts/sync-packs-to-data.sh â€” do not edit manually
# Generated: 2026-02-20T08:34:32Z

"1.1":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-1.01-enforce-sso-with-mfa.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-1.01-enforce-sso-with-mfa.tf"
    excerpts:
      terraform:
        content: |
            # Enforce SSO-only login by disabling local password authentication
            resource "databricks_workspace_conf" "sso_enforcement" {
            custom_config = {
              "enableTokensConfig"     = false
              "enableIpAccessLists"    = var.profile_level >= 2
            }
            }

"1.2":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-1.02-service-principal-security.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-1.02-service-principal-security.tf"
    excerpts:
      terraform:
        content: |
            # Create purpose-specific service principals for automation
            resource "databricks_service_principal" "automation" {
            for_each = var.service_principals
          
            display_name = each.value.display_name
            active       = true
            }
          
            # Grant workspace-level permissions to service principals
            # Each service principal gets CAN_ATTACH_TO on designated clusters only
            resource "databricks_permissions" "service_principal_cluster_usage" {
            for_each = var.service_principals
          
            cluster_policy_id = databricks_cluster_policy.hardened.id
          
            access_control {
              service_principal_name = databricks_service_principal.automation[each.key].application_id
              permission_level       = "CAN_USE"
            }
            }

"1.3":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-1.03-ip-access-lists.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-1.03-ip-access-lists.tf"
    excerpts:
      terraform:
        content: |
            # Allowlist: Restrict workspace access to known corporate IP ranges (L2+)
            resource "databricks_ip_access_list" "allow_corporate" {
            count = var.profile_level >= 2 && length(var.allowed_ip_cidrs) > 0 ? 1 : 0
          
            label        = "HTH - Corporate Network Allow List"
            list_type    = "ALLOW"
            ip_addresses = var.allowed_ip_cidrs
          
            depends_on = [databricks_workspace_conf.sso_enforcement]
            }
          
            # Blocklist: Deny access from known-bad IP ranges (L2+)
            resource "databricks_ip_access_list" "block_bad_ips" {
            count = var.profile_level >= 2 && length(var.blocked_ip_cidrs) > 0 ? 1 : 0
          
            label        = "HTH - Blocked IP Ranges"
            list_type    = "BLOCK"
            ip_addresses = var.blocked_ip_cidrs
          
            depends_on = [databricks_workspace_conf.sso_enforcement]
            }

"2.1":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-2.01-data-governance.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-2.01-data-governance.tf"
    excerpts:
      terraform:
        content: |
            # Unity Catalog grants are managed via SQL statements.
            # This resource configures workspace-level Unity Catalog settings.
            resource "databricks_workspace_conf" "unity_catalog_governance" {
            custom_config = {
              "enableUnityCatalog" = true
            }
            }
          
            # Restrict catalog creation to admins only via SQL global config
            resource "databricks_sql_global_config" "governance" {
            security_policy = "DATA_ACCESS_CONTROL"
          
            data_access_config = {
              "spark.databricks.unityCatalog.enabled" = "true"
            }
            }
  db:
    lang: "sql"
    filename: "hth-databricks-2.01-data-governance.sql"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/db/hth-databricks-2.01-data-governance.sql"
    excerpts:
      db-create-catalog-structure:
        content: |
            -- Create catalogs by environment
            CREATE CATALOG IF NOT EXISTS production;
            CREATE CATALOG IF NOT EXISTS staging;
            CREATE CATALOG IF NOT EXISTS development;
          
            -- Create schemas by domain
            CREATE SCHEMA IF NOT EXISTS production.finance;
            CREATE SCHEMA IF NOT EXISTS production.customer_data;
            CREATE SCHEMA IF NOT EXISTS production.ml_features;
      db-configure-permissions:
        content: |
            -- Grant specific permissions
            GRANT USE CATALOG ON CATALOG production TO `data_analysts`;
            GRANT USE SCHEMA ON SCHEMA production.finance TO `finance_team`;
            GRANT SELECT ON TABLE production.finance.transactions TO `finance_team`;
          
            -- Restrict sensitive tables
            DENY SELECT ON TABLE production.customer_data.pii TO `general_users`;
      db-column-level-security:
        content: |
            -- Create row filter function
            CREATE FUNCTION production.filters.region_filter()
            RETURNS STRING
            RETURN CASE
              WHEN is_account_group_member('us_team') THEN 'region = "US"'
              WHEN is_account_group_member('eu_team') THEN 'region = "EU"'
              ELSE 'FALSE'
            END;
          
            -- Apply to table
            ALTER TABLE production.customer_data.orders
            SET ROW FILTER production.filters.region_filter ON (region);

"2.2":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-2.02-data-masking.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-2.02-data-masking.tf"
    excerpts:
      terraform:
        content: |
            # Note: Dynamic data masking in Unity Catalog is configured via SQL statements
            # (CREATE FUNCTION + ALTER TABLE ... SET MASK). This control enforces the
            # workspace-level configuration that enables column masking support.
            #
            # The SQL masking functions below should be applied via databricks_sql_query
            # or a separate SQL migration pipeline:
            #
            #   CREATE FUNCTION production.masks.mask_ssn(ssn STRING)
            #   RETURNS STRING
            #   RETURN CASE
            #       WHEN is_account_group_member('pii_admin') THEN ssn
            #       ELSE CONCAT('XXX-XX-', RIGHT(ssn, 4))
            #   END;
            #
            #   ALTER TABLE production.customer_data.customers
            #   ALTER COLUMN ssn SET MASK production.masks.mask_ssn;
          
            # Enable table access control to support column-level masking (L2+)
            resource "databricks_workspace_conf" "data_masking" {
            count = var.profile_level >= 2 ? 1 : 0
          
            custom_config = {
              "enableTableAccessControl" = true
            }
            }
  db:
    lang: "sql"
    filename: "hth-databricks-2.02-data-masking.sql"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/db/hth-databricks-2.02-data-masking.sql"
    excerpts:
      db-create-masking-function:
        content: |
            -- Create masking function for SSN
            CREATE FUNCTION production.masks.mask_ssn(ssn STRING)
            RETURNS STRING
            RETURN CASE
              WHEN is_account_group_member('pii_admin') THEN ssn
              ELSE CONCAT('XXX-XX-', RIGHT(ssn, 4))
            END;
          
            -- Apply mask to column
            ALTER TABLE production.customer_data.customers
            ALTER COLUMN ssn SET MASK production.masks.mask_ssn;

"2.3":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-2.03-audit-logging.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-2.03-audit-logging.tf"
    excerpts:
      terraform:
        content: |
            # Enable system tables for audit log access
            # System tables (system.access.audit) provide comprehensive audit logging
            # for all workspace events including data access, cluster operations, and
            # administrative changes.
            resource "databricks_workspace_conf" "audit_logging" {
            custom_config = {
              "enableDbfsFileBrowser" = false
              "enableExportNotebook"  = var.profile_level >= 3 ? false : true
            }
            }
          
            # Note: Verbose audit log queries should be scheduled via Databricks SQL:
            #
            #   SELECT event_time, user_identity.email as user_email,
            #          action_name, request_params.full_name_arg as table_accessed,
            #          source_ip_address
            #   FROM system.access.audit
            #   WHERE action_name IN ('getTable', 'commandSubmit')
            #     AND event_time > current_timestamp() - INTERVAL 24 HOURS
            #   ORDER BY event_time DESC;
  db:
    lang: "sql"
    filename: "hth-databricks-2.03-audit-logging.sql"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/db/hth-databricks-2.03-audit-logging.sql"
    excerpts:
      db-query-audit-logs:
        content: |
            -- Query data access audit logs
            SELECT
              event_time,
              user_identity.email as user_email,
              action_name,
              request_params.full_name_arg as table_accessed,
              source_ip_address
            FROM system.access.audit
            WHERE action_name IN ('getTable', 'commandSubmit')
              AND event_time > current_timestamp() - INTERVAL 24 HOURS
            ORDER BY event_time DESC;

"3.1":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-3.01-cluster-policies.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-3.01-cluster-policies.tf"
    excerpts:
      terraform:
        content: |
            # Hardened cluster policy enforcing approved runtimes, node types,
            # auto-termination, and init script restrictions
            resource "databricks_cluster_policy" "hardened" {
            name = "HTH - Hardened Cluster Policy"
          
            definition = jsonencode({
              "spark_version" = {
                "type"   = "allowlist"
                "values" = var.allowed_spark_versions
              }
              "node_type_id" = {
                "type"   = "allowlist"
                "values" = var.allowed_node_types
              }
              "autotermination_minutes" = {
                "type"         = "range"
                "minValue"     = 10
                "maxValue"     = var.autotermination_minutes_max
                "defaultValue" = var.autotermination_minutes_default
              }
              "custom_tags.Environment" = {
                "type"  = "fixed"
                "value" = "production"
              }
              "custom_tags.ManagedBy" = {
                "type"  = "fixed"
                "value" = "howtoharden"
              }
              "init_scripts" = {
                "type"  = "fixed"
                "value" = []
              }
              "enable_elastic_disk" = {
                "type"         = "fixed"
                "value"        = true
                "hidden"       = true
              }
            })
            }
          
            # Grant CAN_USE on the hardened policy to all users
            resource "databricks_permissions" "cluster_policy_usage" {
            cluster_policy_id = databricks_cluster_policy.hardened.id
          
            access_control {
              group_name       = "users"
              permission_level = "CAN_USE"
            }
            }

"3.2":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-3.02-network-isolation.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-3.02-network-isolation.tf"
    excerpts:
      terraform:
        content: |
            # Enforce no public IP addresses on cluster nodes (L2+)
            # This adds a cluster policy overlay that prevents public IP assignment
            resource "databricks_cluster_policy" "network_isolation" {
            count = var.profile_level >= 2 ? 1 : 0
          
            name = "HTH - Network Isolation Policy"
          
            definition = jsonencode({
              "enable_local_disk_encryption" = {
                "type"  = "fixed"
                "value" = true
              }
              "azure_attributes.availability" = {
                "type"         = "allowlist"
                "values"       = ["ON_DEMAND_AZURE"]
                "defaultValue" = "ON_DEMAND_AZURE"
              }
              "custom_tags.NetworkIsolation" = {
                "type"  = "fixed"
                "value" = "enabled"
              }
            })
            }
          
            # Grant CAN_USE on the network isolation policy (L2+)
            resource "databricks_permissions" "network_isolation_usage" {
            count = var.profile_level >= 2 ? 1 : 0
          
            cluster_policy_id = databricks_cluster_policy.network_isolation[0].id
          
            access_control {
              group_name       = "users"
              permission_level = "CAN_USE"
            }
            }
      terraform-account-level:
        content: |
            # Account-level: Private workspace deployment with VPC isolation
            resource "databricks_mws_workspaces" "this" {
            account_id      = var.databricks_account_id
            workspace_name  = "secure-workspace"
            deployment_name = "secure"
          
            aws_region = var.region
          
            network_id = databricks_mws_networks.this.network_id
          
            # Private configuration
            private_access_settings_id = databricks_mws_private_access_settings.this.private_access_settings_id
            }
          
            resource "databricks_mws_private_access_settings" "this" {
            private_access_settings_name = "secure-pas"
            region                       = var.region
            public_access_enabled        = false
            }

"4.1":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-4.01-secret-scopes.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-4.01-secret-scopes.tf"
    excerpts:
      terraform:
        content: |
            # Create Databricks-backed secret scopes for credential storage
            resource "databricks_secret_scope" "managed" {
            for_each = var.secret_scopes
          
            name                     = each.key
            initial_manage_principal = each.value.initial_manage_principal
            }
          
            # Grant READ access to the data_engineers group on each secret scope
            resource "databricks_secret_acl" "data_engineers_read" {
            for_each = var.secret_scopes
          
            scope      = databricks_secret_scope.managed[each.key].name
            principal  = "data_engineers"
            permission = "READ"
            }
  cli:
    lang: "bash"
    filename: "hth-databricks-4.01-manage-secret-scopes.sh"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/cli/hth-databricks-4.01-manage-secret-scopes.sh"
    excerpts:
      cli-manage-secret-scopes:
        content: |
            # Create secret scope backed by Databricks
            databricks secrets create-scope --scope production-secrets
          
            # Add secrets
            databricks secrets put --scope production-secrets --key db-password
            databricks secrets put --scope production-secrets --key api-key
          
            # Grant read access to specific group
            databricks secrets put-acl \
            --scope production-secrets \
            --principal data_engineers \
            --permission READ
  sdk:
    lang: "python"
    filename: "hth-databricks-4.01-use-secrets-in-notebook.py"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/sdk/hth-databricks-4.01-use-secrets-in-notebook.py"
    excerpts:
      sdk-use-secrets-in-notebook:
        content: |
            # Access secrets in notebook
            db_password = dbutils.secrets.get(scope="production-secrets", key="db-password")
          
            # Secret is redacted in logs
            print(db_password)  # Shows [REDACTED]

"4.2":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-4.02-external-secret-store.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-4.02-external-secret-store.tf"
    excerpts:
      terraform:
        content: |
            # Azure Key Vault-backed secret scope (L2+, Azure only)
            # Secrets are fetched directly from Key Vault at runtime rather than
            # stored in Databricks, providing centralized secret lifecycle management.
            resource "databricks_secret_scope" "azure_keyvault" {
            count = var.profile_level >= 2 && var.azure_keyvault_resource_id != "" ? 1 : 0
          
            name = "azure-kv-scope"
          
            keyvault_metadata {
              resource_id = var.azure_keyvault_resource_id
              dns_name    = var.azure_keyvault_dns_name
            }
            }
          
            # Note: For AWS deployments, use AWS Secrets Manager or Parameter Store
            # integration via instance profiles and IAM roles on the cluster. Configure
            # the cluster policy to enforce the required instance profile ARN.
            #
            # For GCP deployments, use Google Secret Manager via workload identity
            # federation configured on the cluster service account.
  cli:
    lang: "bash"
    filename: "hth-databricks-4.02-azure-keyvault-scope.sh"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/cli/hth-databricks-4.02-azure-keyvault-scope.sh"
    excerpts:
      cli-azure-keyvault-scope:
        content: |
            # Create Key Vault-backed secret scope
            databricks secrets create-scope \
            --scope azure-kv-scope \
            --scope-backend-type AZURE_KEYVAULT \
            --resource-id /subscriptions/.../resourceGroups/.../providers/Microsoft.KeyVault/vaults/my-vault \
            --dns-name https://my-vault.vault.azure.net/

"5.1":
  terraform:
    lang: "hcl"
    filename: "hth-databricks-5.01-security-monitoring.tf"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/terraform/hth-databricks-5.01-security-monitoring.tf"
    excerpts:
      terraform:
        content: |
            # Workspace configuration for security monitoring
            # Disables risky features that complicate audit trails
            resource "databricks_workspace_conf" "security_monitoring" {
            custom_config = {
              # Disable DBFS file browser to prevent unaudited file access
              "enableDbfsFileBrowser" = false
          
              # Restrict notebook export to prevent data exfiltration (L3)
              "enableExportNotebook" = var.profile_level >= 3 ? false : true
          
              # Disable results download for non-admin users (L2+)
              "enableResultsDownloading" = var.profile_level >= 2 ? false : true
            }
            }
          
            # Note: Detection queries should be scheduled as Databricks SQL alerts:
            #
            # Bulk data access detection:
            #   SELECT user_identity.email, request_params.full_name_arg as table_name,
            #          COUNT(*) as access_count
            #   FROM system.access.audit
            #   WHERE action_name = 'commandSubmit'
            #     AND event_time > current_timestamp() - INTERVAL 1 HOUR
            #   GROUP BY user_identity.email, request_params.full_name_arg
            #   HAVING COUNT(*) > 100;
            #
            # Unusual export detection:
            #   SELECT * FROM system.access.audit
            #   WHERE action_name IN ('downloadResults', 'exportResults')
            #     AND event_time > current_timestamp() - INTERVAL 24 HOURS;
            #
            # Service principal anomaly detection:
            #   SELECT user_identity.email, source_ip_address, COUNT(*) as request_count
            #   FROM system.access.audit
            #   WHERE user_identity.email LIKE 'svc-%'
            #     AND event_time > current_timestamp() - INTERVAL 1 HOUR
            #   GROUP BY user_identity.email, source_ip_address;
  db:
    lang: "sql"
    filename: "hth-databricks-5.01-security-monitoring.sql"
    source_url: "https://github.com/grcengineering/how-to-harden/blob/main/packs/databricks/db/hth-databricks-5.01-security-monitoring.sql"
    excerpts:
      db-detect-bulk-access:
        content: |
            -- Detect bulk data access (>100 queries/hour to a single table)
            SELECT
              user_identity.email,
              request_params.full_name_arg as table_name,
              COUNT(*) as access_count
            FROM system.access.audit
            WHERE action_name = 'commandSubmit'
              AND event_time > current_timestamp() - INTERVAL 1 HOUR
            GROUP BY user_identity.email, request_params.full_name_arg
            HAVING COUNT(*) > 100;
      db-detect-unusual-exports:
        content: |
            -- Detect unusual export operations (last 24 hours)
            SELECT *
            FROM system.access.audit
            WHERE action_name IN ('downloadResults', 'exportResults')
              AND event_time > current_timestamp() - INTERVAL 24 HOURS
            ORDER BY event_time DESC;
      db-detect-service-principal-anomalies:
        content: |
            -- Detect service principal anomalies (access from untrusted IPs)
            SELECT
              user_identity.email,
              source_ip_address,
              COUNT(*) as request_count
            FROM system.access.audit
            WHERE user_identity.email LIKE 'svc-%'
              AND source_ip_address NOT IN (SELECT ip FROM trusted_ips)
              AND event_time > current_timestamp() - INTERVAL 1 HOUR
            GROUP BY user_identity.email, source_ip_address;

